{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comandos\n",
    "#1 - variavel = dataframe[['atributo_1','atributo_2']]\n",
    "#2 - X_train, X_test, y_train, y_test = train_test_split(base_de_variaveis_independentes(X),base_de_variavel_dependente(y), test_size=tamanho_da_base_de_teste(De 0 a 1), random_state=42)\n",
    "#3 - classificador.score(base_de_teste)\n",
    "#4 -\n",
    "#def nome_da_função(parâmetros):\n",
    "#   código da função. Aqui, a função vai receber como parâmetro uma linha do dataframe, vai comparar os atributos, que no caso são o valor predito e o valor real,\n",
    "#   e vai retornar se eles são iguais. Os nomes dos campos serão os definidos lá embaixo, no dataframe criado com os valores preditos e os reais.\n",
    "#   Para retornar o valor, use o comando return.\n",
    "#Para chamar a função, use o comando apply. \n",
    "#Exemplo: \n",
    "#df['acerto'] = df.apply(nome_da_função, axis = 1)\n",
    "#Assim, para cada linha de df, ele vai chamar a função nome_da_função e colocar o retorno no atributo acerto. criado especificamente para isso.\n",
    "#Portanto, serão enviados para nome_da_função cada valor predito e cada valor real. Se forem iguais, \n",
    "#o código que você criou vai retornar 1 (ou True, ou o que quer que você queira) para o atributo acerto para aquela linha.\n",
    "#No final, você vai ter um dataframe como o seguinte:\n",
    "#Predito Real Acerto\n",
    "#   0      0    1 (True)\n",
    "#   1      0    0 (False)\n",
    "#   1      1    1 (True)\n",
    "#Então, você deve contar quantos verdadeiros existem e mostrar a porcentagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns]\n",
      "0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "Name: target, Length: 150, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "#Comando extra para baixar os dados:\n",
    "X, y = datasets.load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "# print para tentar visualizar \n",
    "print(X)\n",
    "\n",
    "variavelY = y\n",
    "print(variavelY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     petal length (cm)  petal width (cm)\n",
      "0                  1.4               0.2\n",
      "1                  1.4               0.2\n",
      "2                  1.3               0.2\n",
      "3                  1.5               0.2\n",
      "4                  1.4               0.2\n",
      "..                 ...               ...\n",
      "145                5.2               2.3\n",
      "146                5.0               1.9\n",
      "147                5.2               2.0\n",
      "148                5.4               2.3\n",
      "149                5.1               1.8\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1 - Defina as variáveis independentes usando apenas as informações das pétalas\n",
    "petalasFlor = X[['petal length (cm)', 'petal width (cm)']]\n",
    "\n",
    "print(petalasFlor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Defina a base de treino como 70% dos exemplos\n",
    "\n",
    "treinoX, testeX, treinoY, testeY = train_test_split(petalasFlor, variavelY, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 3 - Mostre a acurácia do modelo a partir da base de teste usando o sklearn\n",
    "\n",
    "# Treinar o modelo de classificação\n",
    "modelo = DecisionTreeClassifier() # instanciando a classe \n",
    "modelo.fit(treinoX, treinoY)\n",
    "\n",
    "# Fazer previsões usando o modelo treinado nos dados de teste -> literalente visões da raven\n",
    "previsoes = modelo.predict(testeX)\n",
    "\n",
    "# Calculando a acurácia do modelo -> aqui faz direto (como forma de teste)\n",
    "from sklearn.metrics import accuracy_score # importando a função \n",
    "acuracia = accuracy_score(testeY, previsoes)\n",
    "\n",
    "print(\"Acurácia do modelo:\", acuracia) # acuracia = numero de vezes que a IA acertou a partir do treinamento de previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia manual do modelo: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Mostre a acurácia do modelo a partir da base de teste manualmente. Para isso, crie uma função.\n",
    "def calcularAcuraciaManualmente(previsoes, testeY):\n",
    "    # Inicializa o contador de previsões corretas\n",
    "    corretas = 0\n",
    "    \n",
    "    # Itera sobre cada previsão e rótulo verdadeiro\n",
    "    for predito, verdadeiro in zip(previsoes, testeY):\n",
    "        # Verifica se a previsão é igual ao rótulo verdadeiro\n",
    "        if predito == verdadeiro:\n",
    "            corretas += 1\n",
    "    \n",
    "    # Calcula a acurácia como o número de previsões corretas dividido pelo total de previsões\n",
    "    acuracia = corretas / len(testeY)\n",
    "    \n",
    "    return acuracia\n",
    "\n",
    "acuraciaManual = calcularAcuraciaManualmente(previsoes, testeY)\n",
    "print(\"Acurácia manual do modelo:\", acuraciaManual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)\n",
      "0                  5.1               3.5\n",
      "1                  4.9               3.0\n",
      "2                  4.7               3.2\n",
      "3                  4.6               3.1\n",
      "4                  5.0               3.6\n",
      "..                 ...               ...\n",
      "145                6.7               3.0\n",
      "146                6.3               2.5\n",
      "147                6.5               3.0\n",
      "148                6.2               3.4\n",
      "149                5.9               3.0\n",
      "\n",
      "[150 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 5 - Defina as variáveis independentes usando apenas as informações das sépalas\n",
    "sepalasFlor = X[['sepal length (cm)', 'sepal width (cm)']]\n",
    "\n",
    "print(sepalasFlor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Defina a base de treino como 70% dos exemplos\n",
    "\n",
    "treinoX2, testeX2, treinoY2, testeY2 = train_test_split(sepalasFlor, variavelY, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# 7 - Mostre a acurácia do modelo a partir da base de teste usando o sklearn\n",
    "\n",
    "# Treinar o modelo de classificação\n",
    "modelo2 = DecisionTreeClassifier() # instanciando a classe \n",
    "modelo2.fit(treinoX2, treinoY2)\n",
    "\n",
    "# Fazer previsões usando o modelo treinado nos dados de teste -> literalente visões da raven\n",
    "previsoes2 = modelo2.predict(testeX2)\n",
    "\n",
    "# Calculando a acurácia do modelo -> aqui faz direto (como forma de teste)\n",
    "from sklearn.metrics import accuracy_score # importando a função \n",
    "acuracia2 = accuracy_score(testeY2, previsoes2)\n",
    "\n",
    "print(\"Acurácia do modelo:\", acuracia2) # acuracia = numero de vezes que a IA acertou a partir do treinamento de previsões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia manual do modelo: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# Chamando a função agora para as sepalas\n",
    "acuraciaManual = calcularAcuraciaManualmente(previsoes2, testeY2)\n",
    "print(\"Acurácia manual do modelo:\", acuraciaManual)\n",
    "\n",
    "# 9 - Compare os dois modelos criados. Qual apresentou melhor acurácia?\n",
    "## A de melhor acuracia foi a das petálas pois acertou todas, tendo 1 de acurácia, enquanto as sepalas teve 0.6 de acuracia\n",
    "# acertando 2 de 3, por exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 - Defina as variáveis independentes usando apenas as informações das pétalas - Comando 1\n",
    "#2 - Defina a base de treino como 70% dos exemplos - Comando 2.\n",
    "#3 - Mostre a acurácia do modelo a partir da base de teste usando o sklearn. Comando 3\n",
    "#4 - Mostre a acurácia do modelo a partir da base de teste manualmente. Para isso, crie uma função. Comando 4.\n",
    "#5 - Defina as variáveis independentes usando apenas as informações das sépalas. Comando 1\n",
    "#6 - Defina a base de treino como 70% dos exemplos. Comando 2.\n",
    "#7 - Mostre a acurácia do modelo a partir da base de teste usando o sklearn. Comando 3.\n",
    "#8 - Mostre a acurácia do modelo a partir da base de teste manualmente.  Para isso, crie uma função. Comando 4.\n",
    "#9 - Compare os dois modelos criados. Qual apresentou melhor acurácia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
